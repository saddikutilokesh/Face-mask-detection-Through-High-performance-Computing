{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wgpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "incDh8xJBENC"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpWUR191BH6A",
        "outputId": "ccc2dd99-a98b-4d51-ec03-88973ebdd612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSm3L55JBMle",
        "outputId": "a8593dfc-58a6-4170-ec37-910c5f989b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMFfZXXvBe_F",
        "outputId": "3936dcc6-d901-4636-e165-31432781c6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8VoseCZCPtA",
        "outputId": "e160dbce-5f57-478a-ce12-2f7bf3d681f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n",
        "\n",
        "class MvsNM():\n",
        "    IMG_SIZE = 50\n",
        "    mask = \"/content/drive/My Drive/ds/dataset/with_mask\"\n",
        "    nomask = \"/content/drive/My Drive/ds/dataset/without_mask\"\n",
        "    LABELS = {mask: 0, nomask: 1}\n",
        "    training_data = []\n",
        "\n",
        "    mask_count = 0\n",
        "    nomask_count = 0\n",
        "\n",
        "    def make_training_data(self):\n",
        "        for label in self.LABELS:\n",
        "            print(label)\n",
        "            for f in tqdm(os.listdir(label)):\n",
        "                if \"jpg\" in f:\n",
        "                    try:\n",
        "                        path = os.path.join(label, f)\n",
        "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
        "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  \n",
        "                        if label == self.mask:\n",
        "                            self.mask_count += 1\n",
        "                        elif label == self.nomask:\n",
        "                            self.nomask_count += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        pass\n",
        "                        #print(label, f, str(e))\n",
        "\n",
        "        np.random.shuffle(self.training_data)\n",
        "        np.save(\"training_data.npy\", self.training_data)\n",
        "        print('Mask:',mvsnm.mask_count)\n",
        "        print('No mask:',mvsnm.nomask_count)\n",
        "\n",
        "if REBUILD_DATA:\n",
        "    mvsnm = MvsNM()\n",
        "    mvsnm.make_training_data()\n",
        "\n",
        "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
        "print(len(training_data))\n",
        "\n",
        "import torch\n",
        "\n",
        "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
        "X = X/255.0\n",
        "y = torch.Tensor([i[1] for i in training_data])\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # just run the init of parent class (nn.Module)\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
        "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
        "\n",
        "        x = torch.randn(50,50).view(-1,1,50,50)\n",
        "        self._to_linear = None\n",
        "        self.convs(x)\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
        "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
        "\n",
        "    def convs(self, x):\n",
        "        # max pooling over 2x2\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
        "\n",
        "        if self._to_linear is None:\n",
        "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "net = Net().to(device)\n",
        "\n",
        "print(net)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
        "X = X/255.0\n",
        "y = torch.Tensor([i[1] for i in training_data])\n",
        "\n",
        "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
        "val_size = int(len(X)*VAL_PCT)\n",
        "print(val_size)\n",
        "\n",
        "train_X = X[:-val_size]\n",
        "train_y = y[:-val_size]\n",
        "\n",
        "test_X = X[-val_size:]\n",
        "test_y = y[-val_size:]\n",
        "print(len(train_X), len(test_X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ds/dataset/with_mask\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2605/2605 [16:07<00:00,  2.69it/s]\n",
            "  0%|          | 0/1948 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ds/dataset/without_mask\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1948/1948 [12:49<00:00,  2.53it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mask: 2474\n",
            "No mask: 1948\n",
            "4422\n",
            "Running on the GPU\n",
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n",
            "442\n",
            "3980 442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V_SMA7LTSYI",
        "outputId": "28003898-c295-4b9a-8818-123ae9f26dd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "start_time=time.time()\n",
        "def train(net):\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    for epoch in range(EPOCHS):\n",
        "        for i in range(0, len(train_X), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
        "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
        "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
        "            batch_y = train_y[i:i+BATCH_SIZE]\n",
        "\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            net.zero_grad()\n",
        "\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            outputs = net(batch_X)\n",
        "            loss = loss_function(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()    # Does the update\n",
        "\n",
        "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
        "\n",
        "train(net)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "test_X.to(device)\n",
        "test_y.to(device)\n",
        "\n",
        "def test(net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(test_X))):\n",
        "            real_class = torch.argmax(test_y[i]).to(device)\n",
        "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]\n",
        "            predicted_class = torch.argmax(net_out)\n",
        "\n",
        "            if predicted_class == real_class:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    print(\"Accuracy: \", round(correct/total, 3))\n",
        "\n",
        "test(net)\n",
        "correct = 0\n",
        "total = 0\n",
        "for i in tqdm(range(0, len(test_X), BATCH_SIZE)):\n",
        "\n",
        "    batch_X = test_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50).to(device)\n",
        "    batch_y = test_y[i:i+BATCH_SIZE].to(device)\n",
        "    batch_out = net(batch_X)\n",
        "\n",
        "    out_maxes = [torch.argmax(i) for i in batch_out]\n",
        "    target_maxes = [torch.argmax(i) for i in batch_y]\n",
        "    for i,j in zip(out_maxes, target_maxes):\n",
        "        if i == j:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "print(\"Accuracy: \", round(correct/total, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0. Loss: 0.14336919784545898\n",
            "Epoch: 1. Loss: 0.10330641269683838\n",
            "Epoch: 2. Loss: 0.02647865191102028\n",
            "Epoch: 3. Loss: 0.020516395568847656\n",
            "Epoch: 4. Loss: 0.012019835412502289\n",
            "Epoch: 5. Loss: 0.00792643427848816\n",
            "Epoch: 6. Loss: 0.0009657948976382613\n",
            "Epoch: 7. Loss: 0.016265179961919785\n",
            "Epoch: 8. Loss: 0.00012442746083252132\n",
            "Epoch: 9. Loss: 3.4991669963346794e-05\n",
            "Epoch: 10. Loss: 0.0001611625193618238\n",
            "Epoch: 11. Loss: 0.00025839515728875995\n",
            "Epoch: 12. Loss: 1.2523730219982099e-05\n",
            "Epoch: 13. Loss: 8.403477113461122e-05\n",
            "Epoch: 14. Loss: 1.8811500694937422e-06\n",
            "Epoch: 15. Loss: 6.134577574812283e-07\n",
            "Epoch: 16. Loss: 9.656370821176097e-06\n",
            "Epoch: 17. Loss: 0.0012540564639493823\n",
            "Epoch: 18. Loss: 6.022842171660159e-06\n",
            "Epoch: 19. Loss: 0.003990841563791037\n",
            "Epoch: 20. Loss: 0.0005229134112596512\n",
            "Epoch: 21. Loss: 8.151933172939607e-08\n",
            "Epoch: 22. Loss: 9.99870849227591e-07\n",
            "Epoch: 23. Loss: 1.0781891823796741e-08\n",
            "Epoch: 24. Loss: 3.4340663646048597e-09\n",
            "Epoch: 25. Loss: 9.808611594053218e-07\n",
            "Epoch: 26. Loss: 6.361379973895964e-07\n",
            "Epoch: 27. Loss: 1.760188206390012e-05\n",
            "Epoch: 28. Loss: 3.380004613973142e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 87/442 [00:00<00:00, 867.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 29. Loss: 8.833613840408816e-10\n",
            "--- 36.85317254066467 seconds ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 442/442 [00:00<00:00, 898.14it/s]\n",
            "100%|██████████| 28/28 [00:00<00:00, 349.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.921\n",
            "Accuracy:  0.921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHYSR27kC1jo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfqFKbObNVaf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}